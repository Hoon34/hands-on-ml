{
 "cells": [
  {
   "attachments": {
    "7ee1359e-0939-4dc2-99b1-490e473a4b4e.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAABqCAMAAADHsNtfAAAAgVBMVEX///8AAAB/f3/w8PD7+/vm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\nM2Z8AP4DUG3/6FZgnRgAAAAASUVORK5CYII=\n"
    }
   },
   "cell_type": "markdown",
   "id": "426b4783-bfe0-495f-8612-3059ebd9f55a",
   "metadata": {},
   "source": [
    "> # Customizing1 #\n",
    "\n",
    "### - Custom loss function ###\n",
    "\n",
    "Suppose there are some noise data in train set. Then what loss function we should use? MSE gives to excessive penalty at big error. So it can't make an optimal model. And MAE takes time to convergence since it is generous to outliers. And the model not be trained precisely. Then we can make Huber loss. Let's make it.\n",
    "\n",
    "![huber.png](attachment:7ee1359e-0939-4dc2-99b1-490e473a4b4e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9201dbf-3cf8-49c0-ad13-b3996cf2c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d817667-b75c-453a-8ea6-1fe353bb9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_pred, y_true):\n",
    "    error = y_true - y_pred\n",
    "    smaller = tf.abs(error) <= 1\n",
    "    small_loss = 0.5 * tf.square(error)\n",
    "    big_loss = tf.abs(error) - 0.5\n",
    "    \n",
    "    return tf.where(smaller, small_loss, big_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c7306-0aff-4b29-b2fa-34bb07d1a42f",
   "metadata": {},
   "source": [
    "It's better to return a tensor which has a loss. Then keras can apply class or sample weight when it's needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba12dcc-af8d-4735-b0e2-9df5f8640e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model_A.h5\")\n",
    "model.compile(loss=huber_loss, optimizer='nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53545c5-ee6d-4e87-9db4-7bcd4440a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"huber_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989fcd3-e267-4706-81ed-7f0c276abf55",
   "metadata": {},
   "source": [
    "When keras saves the model, it saves function name. So no matter accurs in saving. More generally, we have to map name and class when load custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e910a6f-d067-4f3f-bfa7-01276919c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"huber_model.h5\", custom_objects={\"huber_loss\" : huber_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e737a58-ae5e-40ab-b4cd-3c60d59ba23e",
   "metadata": {},
   "source": [
    "Threshold of above fuction is set 1. How we do when we need another threshold value? One method is to make a function that can takes a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7444a615-a5cb-4128-a174-95f3f8e01a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber(threshhold=1):\n",
    "    def huber_loss(y_pred, y_true):\n",
    "        error = y_true - y_pred\n",
    "        smaller = tf.abs(error) <= threshhold\n",
    "        small_loss = 0.5 * tf.square(error)\n",
    "        big_loss = threshhold(tf.abs(error) - 0.5*threshhold)\n",
    "    \n",
    "        return tf.where(smaller, small_loss, big_loss)\n",
    "    return huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc59f3c7-37e1-4d9c-b467-0935f651953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber(2.5), optimizer=\"nadam\")\n",
    "model.save(\"huber_threshold.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97352a9d-05d6-41a0-94d9-5c2089cf9638",
   "metadata": {},
   "source": [
    "When saving model, threshold value is not saved. So we should set threshold value when load model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f776819-cc28-4abb-bb15-942718a3b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"huber_threshold.h5\", custom_objects={\"huber_fn\": huber(2.5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301a524-94ac-49cd-b53e-09101ee44fa4",
   "metadata": {},
   "source": [
    "This problem is solved by keras loss class and config method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b00fa45e-04e1-4d10-8376-197a26d60334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwrags):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwrags)\n",
    "    def call(self, y_pred, y_true):\n",
    "        error = y_true - y_pred\n",
    "        smaller = tf.abs(error) <= self.threshold\n",
    "        small_loss = 0.5 * tf.square(error)\n",
    "        big_loss = self.threshold(tf.abs(error) - 0.5*self.threshold)\n",
    "        return tf.where(smaller, small_loss, big_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c144699-3c69-4ae5-a3ad-868671b0bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=HuberLoss(2.5), optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d121d8d-aeff-4fc6-8de7-9ea0b8044909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"huber_threshold.h5\", custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d05486-803a-466a-abcf-731a28c3dedc",
   "metadata": {},
   "source": [
    "### - Active function, Initialization, Regularizaiton Customizing ###\n",
    "\n",
    "We can customize most of keras function by similar method. We just write a fuction code with proper input and output. Under code is some examples of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c8b05e-9b2a-4da7-a991-c82818b9362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custoom active function\n",
    "\n",
    "def soft_plus(z):\n",
    "    return tf.math.log(tf.exp(z)+1.0)\n",
    "\n",
    "## custom glorot initialization\n",
    "\n",
    "def cus_glorot(shape, dtype=tf.float32):\n",
    "    std = tf.sqrt(2. / (shape[0]+shape[1]))\n",
    "    return tf.random.normal(shape, stddev=std, dtype=dtype)\n",
    "\n",
    "## custom l1 regularization\n",
    "\n",
    "def cus_l1(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01*weights))\n",
    "\n",
    "## custom positive constraint\n",
    "\n",
    "def cus_positive_weight(weights):\n",
    "    return tf.where(weights < 0 , tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e43ad7-6eb7-4a1a-8bd7-222c4628e56f",
   "metadata": {},
   "source": [
    "These made functions can be used like normal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5fc923f-76c4-4795-9aea-56344c3964d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(30, activation=soft_plus, kernel_initializer=cus_glorot,\n",
    "                          kernel_regularizer=cus_l1, kernel_constraint=cus_positive_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d30a8f-b322-4427-b998-4c3846a46e7a",
   "metadata": {},
   "source": [
    "If the functions have hyperparameter to save with, just inherit proper class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60635a26-342c-4048-a585-b59e8411dfc7",
   "metadata": {},
   "source": [
    "### - Custom Metic\n",
    "\n",
    "Loss and metric are not conceptually different. Loss is used for gradient descent. So it is differentiable at all point. People should not understand this function. In contrary, metric is used for evaluating. So people should understand it easily.\n",
    "\n",
    "But making custom metric is same with making custom loss. Previous huber function works well in metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb3f811-4248-405f-93ab-4af38d848f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[huber(2.0)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
