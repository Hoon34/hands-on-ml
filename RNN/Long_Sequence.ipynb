{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fad9350-bb2b-487d-bbad-6d8c660331e4",
   "metadata": {},
   "source": [
    "> # Long sequence\n",
    "\n",
    "Train with long sequence can have long time and be unstable. And model can forget some steps at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8419bd3d-e59c-41ec-ac14-c7e9af175360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8a562-af9b-4a59-9538-4b2b2c5907ba",
   "metadata": {},
   "source": [
    "- Unstable gradient\n",
    "\n",
    "We can use some methods in RNN that we used to solve gradient problem in DNN. But activaiton function that doesn't converge is not a good idea. Output and gradient can explode. So we can use tanh activation function. And gradient clipping also can be good solution to prevent gradient exploding.\n",
    "\n",
    "RNN can't effectively use batch normalization method. We can use it by adding batch normalization layer atmemory cell. But it doesn't make better score. Another method that fit better is layer normalization. This normalize about feature dimension. One advantage is that it can independently calculate statistics of each time step. This means it operates same both train and test. Layer noramlization train one scale and moving parameter at each input. It is used after linear combination of input and hidden states.\n",
    "\n",
    "To use it in RNN, we have to define user memory cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f81b8baf-6d21-41c9-8162-3d17a800c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwarg):\n",
    "        super().__init__(**kwarg)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92feb8a4-fb5b-4024-8cee-a8dc9b353420",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab5ff9-2443-4043-aac6-f8a07174a62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
